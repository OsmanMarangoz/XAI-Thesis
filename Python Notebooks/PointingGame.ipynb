{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EVALUIERUNG VON XAI-METHODEN MITTELS DER \"POINTING GAME\"-ANALYSE\n",
        "\n",
        "Dieses Skript implementiert die \"Pointing Game\"-Analyse, eine Methode zur Bewertung der\n",
        "Lokalisierungsgenauigkeit von XAI-Methoden. Das Ziel ist es zu überprüfen, ob der von einer XAI-Methode als am\n",
        "relevantesten identifizierte Bildpunkt tatsächlich auf dem zu klassifizierenden Objekt liegt.\n",
        "\n",
        "Der Prozess umfasst folgende Schritte:\n",
        "1.  **Laden von Modell und Daten:** Ein vortrainiertes VGG16-Modell sowie ein Testdatensatz\n",
        "    (z.B. ImageNette) werden geladen.\n",
        "2.  **Generierung von Erklärungen:** Für jedes Bild im Testdatensatz werden mithilfe der\n",
        "    \"innvestigate\"-Bibliothek Relevanz-Heatmaps (Erklärungen) für eine Auswahl an XAI-Methoden\n",
        "    (z.B. Gradient, LRP, Guided Backprop) berechnet.\n",
        "3.  **Laden von Segmentierungsmasken:** Für jedes Testbild wird die zugehörige Ground-Truth-\n",
        "    Segmentierungsmaske geladen, die die exakte Position des Objekts im Bild markiert.\n",
        "4.  **Durchführung des \"Pointing Game\":** Für jede generierte Heatmap wird der Punkt mit der höchsten\n",
        "    Relevanz ermittelt (argmax). Anschließend wird überprüft, ob die Koordinaten dieses Punktes\n",
        "    innerhalb der Segmentierungsmaske des Objekts liegen.\n",
        "5.  **Berechnung des Scores:** Ein Treffer (\"Hit\") wird gezählt, wenn der relevanteste Punkt auf dem Objekt\n",
        "    liegt. Der finale \"Pointing Game Score\" für jede XAI-Methode ist der prozentuale Anteil der\n",
        "    Treffer an der Gesamtzahl der Testbilder.\n",
        "6.  **Ergebnisauswertung:** Die Scores werden tabellarisch ausgegeben, um die XAI-Methoden hinsichtlich\n",
        "    ihrer Fähigkeit, das korrekte Objekt zu lokalisieren, quantitativ zu vergleichen.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YtlmDvHHNM5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f_LGFc7KlwV"
      },
      "outputs": [],
      "source": [
        "import imp\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "from types import SimpleNamespace\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.backend\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow.compat.v1 as tf1\n",
        "tf1.reset_default_graph()\n",
        "tf.compat.v1.disable_eager_execution() #must have für innvestigate weil für tf1 gebaut.\n",
        "\n",
        "#laden der lib lokal für eigene anpassungen, sonst kann library von pip install genutzt.\n",
        "example_utils_path = \"/mnt/data/environments/innvestigate/examples/\"\n",
        "example_utils_path = os.path.abspath(example_utils_path)\n",
        "\n",
        "if example_utils_path not in sys.path:\n",
        "    sys.path.append(example_utils_path)\n",
        "\n",
        "dev_path = \"/mnt/data/environments/innvestigate/src/\"\n",
        "\n",
        "if dev_path not in sys.path:\n",
        "    sys.path.insert(0, dev_path) # Fügt Ihren Entwicklungspfad an den Anfang der Liste ein\n",
        "\n",
        "#innvestigate imports\n",
        "import innvestigate\n",
        "import innvestigate.tools.perturbate\n",
        "import innvestigate.utils as iutils\n",
        "#examples utils1 help funcitons\n",
        "import utils1 as eutils\n",
        "import utils1.imagenet as imagenetutils\n",
        "from innvestigate.tools.perturbate import Perturbation, PerturbationAnalysis\n",
        "\n",
        "#VRAM Mem growth\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "if physical_gpus:\n",
        "    tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
        "\n",
        "print(f\"sys.path[0]: {sys.path[0]}\") # Sollte jetzt Ihr dev_path sein\n",
        "print(f\"Geladenes innvestigate Paket: {innvestigate.__file__}\")\n",
        "print(f\"Geladene perturbate.py: {innvestigate.tools.perturbate.__file__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = './models/VGG16 MODELS/vgg16_imagenet_a96_va63_l150_vl278.h5'\n",
        "#MODEL_PATH = './models/vgg16_cifar10_a87_va85_l378_vl448.h5'\n",
        "#MODEL_PATH = './models/VGG16 MODELS/vgg16_oxfordpets_a69_va60_l1310_vl1920.h5'\n",
        "#MODEL_PATH = './models/VGG16 MODELS/vgg16_caltech101_a68_va56_l1277_vl2056.h5'\n",
        "#MODEL_PATH = './models/vgg16_sun397_a79_va58_l761_vl1768.h5'\n",
        "\n",
        "model = tf.compat.v1.keras.models.load_model(MODEL_PATH, compile=False)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])#learning rate should be set to 0.0001 as models are trained that way\n"
      ],
      "metadata": {
        "id": "9RZgWyLkKoUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "(ds_test, ds_train), ds_info = tfds.load(\n",
        "    'imagenette/full-size-v2',\n",
        "    split=['validation', 'train'],\n",
        "    shuffle_files=False,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        "    data_dir='/mnt/data/datasets'\n",
        ")\n",
        "\n",
        "def preprocess_vgg(image, label):\n",
        "    # Resize to VGG16's expected input size\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    # Apply VGG16 preprocessing (convert to float32 + mean subtraction)\n",
        "    image = preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "def one_hot_encode_labels(image, label):\n",
        "    # Apply one-hot encoding to the label\n",
        "    label = tf.one_hot(label, 10)\n",
        "    return image, label\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "#ds_train = ds_train.take(1000)\n",
        "ds_test = ds_test.take(800)\n",
        "\n",
        "\n",
        "# Apply preprocessing to both datasets\n",
        "ds_train = ds_train.map(preprocess_vgg)\n",
        "ds_train = ds_train.shuffle(buffer_size=1000)\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(preprocess_vgg,deterministic=True)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply one-hot encoding within the data pipeline\n",
        "ds_train = ds_train.map(one_hot_encode_labels)\n",
        "ds_test = ds_test.map(one_hot_encode_labels)\n",
        "\n",
        "# Convert TF Dataset to NumPy arrays by iterating and concatenating batches\n",
        "train_images_list = []\n",
        "train_labels_list = []\n",
        "\n",
        "# Iterate through the dataset and append batches to the lists, hier übergebe ich ob train oder validation data für die validierung/perturbationsanalyse benutzt werden soll.\n",
        "# if ds_train, dann subset zuerst davon erstellen.\n",
        "for images, labels in tfds.as_numpy(ds_test):\n",
        "    train_images_list.append(images)\n",
        "    train_labels_list.append(labels)\n",
        "\n",
        "# Concatenate the batches into single NumPy arrays\n",
        "train_images = np.concatenate(train_images_list, axis=0)\n",
        "train_labels = np.concatenate(train_labels_list, axis=0)\n",
        "\n",
        "#test_sample für perturbationsanalyse\n",
        "test_sample = np.copy(train_images[0:1])\n",
        "print(\"Test sample shape: {}\".format(test_sample.shape))\n",
        "print(\"generator variables data type:\",(test_sample.dtype))\n",
        "\n",
        "#check ob tensor alle richtig.\n",
        "print(\"Train images shape:\", train_images.shape)  # Should be (N, 224, 224, 3)\n",
        "print(\"Train images dtype:\", train_images.dtype)  # Should be float32\n",
        "print(\"Train labels shape:\", train_labels.shape)  # Should be (N,)\n",
        "print(\"Train labels dtype:\", train_labels.dtype)  # Should be float32\n"
      ],
      "metadata": {
        "id": "-4dGAw9UKqm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = iutils.sequence.BatchSequence([train_images, train_labels], batch_size=16)\n",
        "\n",
        "perturbation_function = \"uniform\"\n",
        "region_shape = (56, 56)\n",
        "steps = 16\n",
        "regions_per_step = 1  # Perturbate 1 region per step\n",
        "\n",
        "# set postprocessing parameter\n",
        "channels_first = keras.backend.image_data_format() == \"channels_first\"\n",
        "color_conversion = \"BGRtoRGB\"  # keras.applications use BGR format\n",
        "\n",
        "# VGG16 preprocess_input erwartet Eingaben im [0, 255] Bereich (RGB),\n",
        "# konvertiert intern zu BGR und subtrahiert Mittelwerte [103.939, 116.779, 123.68] (BGR)\n",
        "VGG_MEANS_BGR = np.array([103.939, 116.779, 123.68]) # B, G, R\n",
        "\n",
        "def revert_preprocessing_vgg(X_preprocessed_bgr):\n",
        "    X_bgr = X_preprocessed_bgr + VGG_MEANS_BGR\n",
        "    X_rgb = X_bgr[..., ::-1] # Kanalreihenfolge umkehren (BGR zu RGB)\n",
        "    return X_rgb\n",
        "\n",
        "#vgg revert postprocessing scale to\n",
        "def input_postprocessing_vgg(X_vgg_preprocessed):\n",
        "    X_reverted_to_0_255_rgb = revert_preprocessing_vgg(X_vgg_preprocessed)\n",
        "    X_reverted_to_0_255_rgb = np.clip(X_reverted_to_0_255_rgb, 0, 255)\n",
        "    return X_reverted_to_0_255_rgb / 255.0\n",
        "\n",
        "input_range = (-128, 128)  # format used by keras.applications\n",
        "noise_scale = (input_range[1] - input_range[0]) * 0.1\n",
        "\n",
        "ri = input_range[0]  # reference input\n",
        "\n",
        "# Configure analysis methods and properties\n",
        "# fmt: off\n",
        "methods = [\n",
        "    # NAME                    OPT.PARAMS                POSTPROC FXN            TITLE\n",
        "    # Show input\n",
        "    (\"input\",               {},                         input_postprocessing_vgg,   \"Input\"),\n",
        "    # Random\n",
        "    (\"random\",              {},                         imagenetutils.graymap,     \"Random\"),\n",
        "    # Function\n",
        "    (\"gradient\",            {\"postprocess\": \"abs\"},     imagenetutils.graymap,     \"Gradient\"),\n",
        "    (\"smoothgrad\",\n",
        "                            {\"noise_scale\": noise_scale,\n",
        "                             \"postprocess\": \"square\"},  imagenetutils.graymap,     \"SmoothGrad\"),\n",
        "    # Signal\n",
        "    (\"deconvnet\",           {},                         imagenetutils.bk_proj,     \"Deconvnet\"),\n",
        "    (\"guided_backprop\",     {},                         imagenetutils.bk_proj,     \"Guided Backprop\"),\n",
        "    # Interaction\n",
        "    (\"deep_taylor.bounded\", {\"low\": input_range[0],\n",
        "                             \"high\": input_range[1]},   imagenetutils.heatmap,     \"DeepTaylor\"),\n",
        "    (\"input_t_gradient\",    {},                         imagenetutils.heatmap,     \"Input * Gradient\"),\n",
        "    (\"integrated_gradients\",{\"reference_inputs\": ri},   imagenetutils.heatmap,     \"Integrated Gradients\"),\n",
        "    (\"lrp.z\",               {},                         imagenetutils.heatmap,     \"LRP-Z\"),\n",
        "    (\"lrp.epsilon\",         {\"epsilon\": 1},             imagenetutils.heatmap,     \"LRP-Epsilon\"),\n",
        "]\n",
        "# fmt: on\n",
        "\n",
        "# Select methods of your choice\n",
        "selected_methods_indices = [2,3,4,5,6,7,8,9,10]\n",
        "selected_methods = [methods[i] for i in selected_methods_indices]\n",
        "print(f'Using method(s) \"{[method[0] for method in selected_methods]}\".')"
      ],
      "metadata": {
        "id": "gQh7ZnkFKwaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model without softmax\n",
        "model_wo_softmax = innvestigate.model_wo_softmax(model)\n",
        "\n",
        "analyzers = []\n",
        "for method in selected_methods:\n",
        "    try:\n",
        "        analyzer = innvestigate.create_analyzer(\n",
        "            method[0],          # analysis method identifier\n",
        "            model_wo_softmax,   # model without softmax output\n",
        "            **method[1]\n",
        "        )                     # optional analysis parameters\n",
        "    except innvestigate.NotAnalyzeableModelException:\n",
        "        # Not all methods work with all models.\n",
        "        analyzer = None\n",
        "    analyzers.append(analyzer)\n",
        "\n",
        "print(\"Initial analyzers list created.\")\n",
        "\n",
        "# wenn random ausgewählt ist, find the index of the 'random' method in your list\n",
        "method_names = [m[0] for m in selected_methods]\n",
        "try:\n",
        "    random_method_index = method_names.index(\"random\")\n",
        "\n",
        "    # Create a very simple object that just has an .analyze() method\n",
        "    correct_random_analyzer = SimpleNamespace(\n",
        "        analyze=lambda x: np.random.rand(*x.shape)\n",
        "    )\n",
        "\n",
        "    # It replaces the broken analyzer in your list with the correct one.\n",
        "    print(\"Applying simple fix for 'random' analyzer...\")\n",
        "    analyzers[random_method_index] = correct_random_analyzer\n",
        "\n",
        "except ValueError:\n",
        "    # This will run if 'random' is not in your selected_methods list\n",
        "    print(\"'random' method not found, no fix applied.\")\n"
      ],
      "metadata": {
        "id": "LZTwIboHLEef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_explanations = {method[0]: [] for method in selected_methods}\n",
        "for method, analyzer in zip(selected_methods, analyzers):\n",
        "    print(f\"Generiere 800 Erklärungen für die Methode: {method[0]}...\")\n",
        "\n",
        "    # Gehe durch jedes der 800 Testbilder\n",
        "    for image in train_images:\n",
        "        # Das Modell erwartet einen \"Batch\" von Bildern, auch wenn es nur eines ist.\n",
        "        # Wir fügen daher eine Dimension hinzu: (224, 224, 3) -> (1, 224, 224, 3)\n",
        "        image_with_batch_dim = image[None, :, :, :]\n",
        "\n",
        "        # Berechne die Erklärung für das einzelne Bild\n",
        "        explanation = analyzer.analyze(image_with_batch_dim)\n",
        "\n",
        "        # Entferne die Batch-Dimension wieder und speichere das Array\n",
        "        # (1, 224, 224, 3) -> (224, 224, 3)\n",
        "        all_explanations[method[0]].append(explanation.squeeze())\n",
        "\n",
        "    # Optional: Wandle die Liste von Arrays in ein einziges großes NumPy-Array um\n",
        "    all_explanations[method[0]] = np.array(all_explanations[method[0]])"
      ],
      "metadata": {
        "id": "mojq0j1dLGnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filename = '/mnt/data/masken/imagenette_test.npy'\n",
        "final_masks_array = None\n",
        "try:\n",
        "    original_masks = np.load(mask_filename, allow_pickle=True)\n",
        "    print(f\" Original-Masken-Datei '{mask_filename}' geladen.\")\n",
        "\n",
        "    target_size = (224, 224)\n",
        "    resized_masks = [cv2.resize(mask.astype(np.uint8), target_size, interpolation=cv2.INTER_NEAREST) for mask in original_masks]\n",
        "    final_masks_array = np.array(resized_masks, dtype=np.uint8)\n",
        "    print(f\"Skalierung abgeschlossen. Neues Masken-Array hat die Form: {final_masks_array.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"FEHLER beim Laden der Masken: {e}\")\n",
        "\n",
        "if final_masks_array is not None and 'all_explanations' in locals():\n",
        "    pointing_game_scores = {}\n",
        "    print(\"\\n--- Starte Pointing Game Auswertung ---\")\n",
        "\n",
        "    for method_name, explanations in all_explanations.items():\n",
        "        hit_counter = 0\n",
        "        total_images = len(explanations)\n",
        "        print(f\"\\n--- Methode: {method_name} ---\")\n",
        "\n",
        "        for i in range(total_images):\n",
        "            explanation_map_color = explanations[i]\n",
        "            segmentation_mask = final_masks_array[i]\n",
        "\n",
        "            if explanation_map_color.ndim == 3 and explanation_map_color.shape[-1] > 1:\n",
        "                explanation_map_gray = np.mean(explanation_map_color, axis=-1)\n",
        "            else:\n",
        "                explanation_map_gray = np.squeeze(explanation_map_color)\n",
        "\n",
        "            argmax_flat = np.argmax(explanation_map_gray)\n",
        "            y, x = np.unravel_index(argmax_flat, explanation_map_gray.shape)\n",
        "\n",
        "            is_hit = segmentation_mask[y, x] == 1\n",
        "            if is_hit:\n",
        "                hit_counter += 1\n",
        "\n",
        "        pg_score = (hit_counter / total_images) * 100\n",
        "        pointing_game_scores[method_name] = (pg_score, hit_counter, total_images)\n",
        "        print(f\"Ergebnis für '{method_name}': {hit_counter} von {total_images} Treffern => PG-Score: {pg_score:.2f}%\")\n",
        "\n",
        "\n",
        "    # Sortiere nach dem Score, der an Position 0 des Wert-Tupels ist\n",
        "    sorted_scores = sorted(pointing_game_scores.items(), key=lambda item: item[1][0], reverse=True)\n",
        "\n",
        "    # Gib eine Kopfzeile aus (wird nicht mitkopiert)\n",
        "    print(\"Methode;Hits;Total;Score (%)\")\n",
        "    print(\"-\" * 40) # Visuelle Trennlinie\n",
        "\n",
        "    # Gib die Datenzeilen im CSV-Format (mit Semikolon) aus\n",
        "    for method, (score, hits, total) in sorted_scores:\n",
        "        # Formatiere den Score mit Komma als Dezimaltrennzeichen\n",
        "        score_german_format = f\"{score:.2f}\".replace('.', ',')\n",
        "\n",
        "        # Gib die Zeile mit Semikolon als Trenner aus\n",
        "        print(f\"{method};{hits};{total};{score_german_format}\")\n",
        "\n",
        "elif 'all_explanations' not in locals():\n",
        "     print(\"\\nVariable 'all_explanations' nicht gefunden.\")\n",
        "else:\n",
        "     print(\"\\nMasken konnten nicht geladen werden, Auswertung wird übersprungen.\")"
      ],
      "metadata": {
        "id": "pqQeqRU-LJRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hilfsfunktion zum überprüfen ob die richtigen Masken geladen wurden.\n",
        "try:\n",
        "    ds_plot = ds_test.take(10)\n",
        "    # Deine funktionierende Methode zum Extrahieren der Daten\n",
        "    for images, labels in tfds.as_numpy(ds_plot):\n",
        "        original_images_for_plotting = images\n",
        "        break\n",
        "\n",
        "    print(f\"{len(original_images_for_plotting)} Originalbilder nach deiner Methode geladen und einfach verarbeitet.\")\n",
        "\n",
        "    #masken laden aus npy datei\n",
        "    mask_filename = '/mnt/data/masken/caltech101_test.npy'\n",
        "    original_masks = np.load(mask_filename, allow_pickle=True)\n",
        "\n",
        "    target_size = (224, 224)\n",
        "    resized_masks_for_plotting = []\n",
        "\n",
        "    for mask in original_masks[:10]:\n",
        "        mask_uint8 = mask.astype(np.uint8)\n",
        "        resized_mask = cv2.resize(mask_uint8, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "        resized_masks_for_plotting.append(resized_mask)\n",
        "\n",
        "    final_10_masks = np.array(resized_masks_for_plotting)\n",
        "    print(f\"{len(final_10_masks)} Masken geladen und skaliert.\")\n",
        "\n",
        "    print(\"Erstelle den Plot...\")\n",
        "    num_examples = 10\n",
        "    fig, axes = plt.subplots(num_examples, 2, figsize=(8, 40))\n",
        "    fig.suptitle(\"Finaler Abgleich: Originalbild vs. Maske\", fontsize=16, y=1.0)\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ax1 = axes[i, 0]\n",
        "        ax1.imshow(original_images_for_plotting[i])\n",
        "        ax1.set_title(f\"Originalbild #{i}\")\n",
        "        ax1.axis('off')\n",
        "\n",
        "        ax2 = axes[i, 1]\n",
        "        ax2.imshow(final_10_masks[i], cmap='gray')\n",
        "        ax2.set_title(f\"Skalierte Maske #{i}\")\n",
        "        ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = 'final_image_mask_comparison.png'\n",
        "    plt.savefig(output_filename)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nPlot wurde erfolgreich als '{output_filename}' gespeichert.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nEin Fehler ist aufgetreten: {e}\")"
      ],
      "metadata": {
        "id": "AXxbicW7LV1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}